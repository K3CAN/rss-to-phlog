#!/usr/bin/perl

use XML::Feed;
use LWP::Protocol::https;
use strict;
use Getopt::Std;

getopts('hf:r:e:');
our ($opt_e,$opt_f,$opt_r, $opt_h);       HELP_MESSAGE() if $opt_h; 
my $count;
my $feed_url = defined($opt_f) ? $opt_f : HELP_MESSAGE();   
my $phlogroot = defined($opt_r) ? $opt_r : HELP_MESSAGE();
my $entry_count = defined($opt_e) ? $opt_e : 3;

#------------------------------------

my $feed = XML::Feed->parse(URI->new($feed_url)) or die XML::Feed->errstr;
warn "Found feed titled ", $feed->title, ".\n";

for my $entry ($feed->entries) {
  $count++;
  warn "$count: Found entry titled ", $entry->title, ".\n";
  write_entry($entry->title, $entry->content->body, $entry->issued->ymd,$phlogroot);  
  last if $count == $entry_count;
}

#Notes to myself, because I'll probably forget:
#$feed->entries returns an array containing XML::Feed::Entry objects.
#Those Entry objects then contain "Content" which are XML::Feed::Content objects. 


#Subroutines--------------------------

sub write_entry {
  my ($title,$content,$date,$root) = @_;
  my $filename = getfilename($title,$date,$root);
  $content = strip_html($content);
  if (-e $filename) {
    warn "\tEntry $title already exists\n";
    open (my $phlog, '<',$filename) or die "Found $filename but cannot open it\n";
    my $existing = do {local $/ = undef; <$phlog>};   #slurp in the file as a single string
    if ($content eq $existing) {                      #Could use a hash (MD5) here instead? 
      warn "\tNo changes to $title, skipping\n";
      close $phlog;
      return;
    }
    warn "\tUpdating contents of $title\n";
  }
    open (my $phlog, '>',$filename) or die "failed to create file $filename\n";
    print $phlog $content; warn "\tWriting to  $filename\n";
    close $phlog;
}

sub getfilename {
  my ($title,$date,$root) = @_; 
  $title =~ s/[<>:"\/\\'!|?* \(\)]//g; #Strip out stuff that shouldn't be in a filename
  return($root.substr($date."_".$title, 0,36));
}

#Consider replacing this with HTML::Strip? What about images? 
sub strip_html {        
  $_[0] =~ s/<img[^>]*?src\s*=\s*[""']?([^'"" >]+?)[ '""][^>]*?>/\(http link: $1\)/gi; #replace images with plain text addresses
  $_[0]=~ s/<br\s*\/?>/\n/gi; #replace HTML newlines with \n 
  $_[0] =~ s/<\/?([a-zA-Z]+)[^>]*>//gi; #strip any remaining HTML tags entirely.
  return $_[0];
}

sub HELP_MESSAGE {
print <<EOF;

DESCRIPTION

This program extracts post titles, dates and body content from a blog's
RSS feed and then creates a series of plain files. Each file is titled 
with the date and post title, and the post content is written to the 
file in plain text. 

This was created for the purpose of automatically mirroring a W3 blog
to a gopher server, but could potentially have other uses. 

USAGE

rss-to-phlog [OPTIONS]

The following options are supported: 

-r [path]   Required to specify the destination path
-f [feed]   Required to specify the RSS feed address (in double quotes)
-e [number] Optional to specify the number of RSS entries to review. Default=3.
-h|--help   Print this message and exit. 

Note: Depending on the feed, this script may pull a large number of entries
each  time it is run. You may be able to limit this by requesting a reduced 
feed via API or adjusting the feed settings if you control the feed. 
Setting -e does NOT  change how many entries are received from the RSS feed,
it only determines how many of the received entries are processed. 
EOF
  exit 2;
}